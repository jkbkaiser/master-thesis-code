{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fac367e4-3b1a-42e8-a365-472582345b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import shutil\n",
    "import time\n",
    "import uuid\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datasets import Features, Image, Value, load_dataset\n",
    "from PIL import Image as PILImage\n",
    "from PIL import UnidentifiedImageError\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_ENTRIES = 2\n",
    "\n",
    "DATA_DIR = Path(\"./../../data\")\n",
    "GBIF_DATA_DIR = DATA_DIR / \"gbif\"\n",
    "HF_DATA_DIR = DATA_DIR / \"hf\"\n",
    "HF_IMG_DIR = HF_DATA_DIR / \"images\"\n",
    "\n",
    "HUGGING_FACE_DATASET = \"jkbkaiser/thesis-gbif-raw\"\n",
    "\n",
    "# Ensure the hf image directory is empty\n",
    "if HF_IMG_DIR.exists():\n",
    "    shutil.rmtree(HF_IMG_DIR)\n",
    "HF_IMG_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d598c1a3-60e1-4975-998b-5552d3b67fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func, attempts):\n",
    "    sleep = 5\n",
    "    result = None\n",
    "\n",
    "    for _ in range(attempts):\n",
    "        try:\n",
    "            result = func()\n",
    "            break\n",
    "        except Exception:\n",
    "            time.sleep(sleep)\n",
    "        sleep *= 2\n",
    "\n",
    "    if result is None:\n",
    "        raise Exception(\"Failed after three attempts\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def download_img(ident, img_url, directory_path: Path):\n",
    "    img_filename = f\"{ident}.jpeg\"\n",
    "    response = retry(lambda: requests.get(img_url), 3)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed request with status {response.status_code}\")\n",
    "\n",
    "    fmt = response.headers[\"Content-Type\"]\n",
    "    if fmt not in [\"image/jpeg\", \"image/jpg\"]:\n",
    "        raise Exception(f\"Unsupported img format {fmt}\")\n",
    "\n",
    "    with open(directory_path / img_filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    return img_filename\n",
    "\n",
    "\n",
    "def download_gbif_entry(entry):\n",
    "    identifier = uuid.uuid4()\n",
    "    img_filename = download_img(identifier, entry[\"identifier\"], HF_IMG_DIR)\n",
    "\n",
    "    entry = {\n",
    "        \"file_name\": f\"./images/{img_filename}\",\n",
    "        \"id\": identifier,\n",
    "        \"kingdom_key\": entry[\"kingdomKey\"],\n",
    "        \"phylum_key\": entry[\"phylumKey\"],\n",
    "        \"order_key\": entry[\"orderKey\"],\n",
    "        \"family_key\": entry[\"familyKey\"],\n",
    "        \"genus_key\": entry[\"genusKey\"],\n",
    "        \"scientific_name\": entry[\"scientificName\"],\n",
    "        \"species\": entry[\"species\"],\n",
    "        \"sex\": entry[\"sex\"],\n",
    "        \"life_stage\": entry[\"lifeStage\"],\n",
    "        \"continent\": entry[\"continent\"]\n",
    "    }\n",
    "\n",
    "    return entry\n",
    "\n",
    "\n",
    "def download_gbif_entries_parallel(dataframe, num):\n",
    "    gbif_entry_generator = (entry for _, entry in dataframe.head(num).iterrows())\n",
    "\n",
    "    def safe_download_gbif_entry(entry):\n",
    "        try:\n",
    "            return download_gbif_entry(entry)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {entry}: {e}\")\n",
    "            return None\n",
    "\n",
    "    with ThreadPool() as pool:\n",
    "        with tqdm(total=num) as pbar:\n",
    "            results = []\n",
    "\n",
    "            for result in pool.imap_unordered(\n",
    "                safe_download_gbif_entry, gbif_entry_generator\n",
    "            ):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "                pbar.update(1)\n",
    "    return results\n",
    "\n",
    "\n",
    "def store_csv(data):\n",
    "    header = [\n",
    "        \"file_name\",\n",
    "        \"id\",\n",
    "        \"kingdom_key\",\n",
    "        \"phylum_key\",\n",
    "        \"order_key\",\n",
    "        \"family_key\",\n",
    "        \"genus_key\",\n",
    "        \"scientific_name\",\n",
    "        \"species\",\n",
    "        \"sex\",\n",
    "        \"life_stage\",\n",
    "        \"continent\",\n",
    "    ]\n",
    "    csv_filename = HF_DATA_DIR / \"metadata.csv\"\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for entry in data:\n",
    "            if entry is not None and entry:\n",
    "                writer.writerow(entry)\n",
    "\n",
    "\n",
    "def filter_corrupted_images(ds):\n",
    "    def validate_image(row):\n",
    "        try:\n",
    "            with PILImage.open(row[\"image\"][\"path\"]) as img:\n",
    "                img.verify()  # Check for corrupted files\n",
    "            return True\n",
    "        except (UnidentifiedImageError, IOError):\n",
    "            return False\n",
    "\n",
    "    ds = ds.cast_column(\"image\", Image(decode=False))\n",
    "    ds = ds.filter(validate_image)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def preprocess_entry(entry):\n",
    "    img = entry[\"image\"]\n",
    "    img = img.resize((256, 256))\n",
    "\n",
    "    def crop_center(img):\n",
    "        new_height, new_width = 224, 224\n",
    "        width, height = img.size\n",
    "\n",
    "        left = (width - new_width) / 2\n",
    "        top = (height - new_height) / 2\n",
    "        right = (width + new_width) / 2\n",
    "        bottom = (height + new_height) / 2\n",
    "\n",
    "        img = img.crop((left, top, right, bottom))\n",
    "        return img\n",
    "\n",
    "    img = crop_center(img)\n",
    "    entry[\"image\"] = img\n",
    "    return entry\n",
    "\n",
    "\n",
    "def preprocess_dataset(ds):\n",
    "    features = Features(\n",
    "        {\n",
    "            \"image\": Image(mode=None, decode=True, id=None),\n",
    "            \"id\": Value(dtype=\"string\", id=None),\n",
    "            \"kingdom_key\": Value(dtype=\"uint32\", id=None),\n",
    "            \"phylum_key\": Value(dtype=\"uint32\", id=None),\n",
    "            \"order_key\": Value(dtype=\"uint32\", id=None),\n",
    "            \"family_key\": Value(dtype=\"uint32\", id=None),\n",
    "            \"genus_key\": Value(dtype=\"uint32\", id=None),\n",
    "            \"scientific_name\": Value(dtype=\"string\", id=None),\n",
    "            \"species\": Value(dtype=\"string\", id=None),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ds = ds.cast(features)\n",
    "    ds = ds.map(preprocess_entry)\n",
    "    \n",
    "    def valid_image_shape(row):\n",
    "        return row[\"image\"].mode == \"RGB\"\n",
    "\n",
    "    ds = ds.filter(valid_image_shape)\n",
    "    ds_dict = datasets.DatasetDict({\"data\": ds})\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56ca4463-b815-48e9-b1c5-31fb01c35dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "multimedia = pd.read_csv(GBIF_DATA_DIR / \"multimedia.txt\", sep=\"\\t\")\n",
    "occurrence = pd.read_csv(\n",
    "    GBIF_DATA_DIR / \"occurrence.txt\", sep=\"\\t\", low_memory=False\n",
    ")\n",
    "\n",
    "multimedia = multimedia[\n",
    "    (multimedia[\"format\"] == \"image/jpeg\")\n",
    "    | (multimedia[\"format\"] == \"jpeg\")\n",
    "    | (multimedia[\"format\"] == \"image/png\")\n",
    "]\n",
    "\n",
    "df = multimedia.merge(occurrence, on=\"gbifID\", how=\"inner\")\n",
    "data = download_gbif_entries_parallel(df, NUM_ENTRIES)\n",
    "store_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe18ed-d1a3-443a-a207-7f3262120f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(str(HF_DATA_DIR))[\"train\"]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d93afba9-59fa-4b3a-9605-86e9eaf5e1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccea0c0411214fa3a728694cd2f3e782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = filter_corrupted_images(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11b0dce9-e912-4747-b08a-6f087821203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7fae3a86fe4989856d42d5019316af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/9839 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4371af0d134e66aba163397dddd655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9839 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acc3a94cd96440faf16631161dafa5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9839 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = preprocess_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72dc31b7-75e3-4364-84a3-a62088dd87db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    data: Dataset({\n",
       "        features: ['image', 'id', 'kingdom_key', 'phylum_key', 'order_key', 'family_key', 'genus_key', 'scientific_name', 'species'],\n",
       "        num_rows: 9838\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c588a6d-477f-4aa4-8f57-6f74f8628f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bc1c86444f45c482c1dff111a7e108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e23fe49e54f43f68101d944232f2e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23990c4f135c409a95d77fe950e72c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34e66a0ac0743048c0b43ca5a052289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef5ab1825234ff39d439387a352f3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.push_to_hub(HUGGING_FACE_DATASET, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d090b47-99e2-4b3e-a8f4-3e09977c0867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac170f3-a8dc-463a-82e9-49c40e4d2913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
